# 備忘録（ゼロから作るDeep Learningを読んで）

## 概要

本記事では，ゼロから作るDeep Learningを読んで自分が後から忘れそうなことを点々と記録する．間違ったこと書いていたら，指摘していただけると幸い．

## 3章

- 出力層  
出力層で使用する活性化関数は，回帰問題では恒等関数，2クラス分類問題ではシグモイド関数，多クラス分類問題ではソフトマックス関数を一般的に利用する．
- 恒等関数  
入力をそのまま出力する関数．
- シグモイド関数  


## 4章

- ミニバッチ学習  
MNISTのデータセットは訓練データが60000個あるが，すべてのデータを対象に損失関数の和を求めるのは，少々時間がかかる．それ以外の，数百万，数億のデータを扱う場合，すべてのデータを対象に損失関数を計算するのは現実的ではない．そこで，データの中から一部を選び出し，その一部のデータを全体の「近似」として利用する．訓練データからある枚数だけ抜き出し（ミニバッチ），そのミニバッチごとに学習を行う．MNISTなら，60000枚の訓練データの中から，100枚を無作為に選び出して学習を行う．このような学習手法を，ミニバッチ学習という．
- エポック（epoch）  
学習の時に用いられる単位．1epoch＝学習において訓練データを全て使い切った時の回数に対応する．10000個のデ訓練データに対して100個のミニバッチで学習する場合，SGDを100回繰り返したら，全ての訓練データを“見た”ことになる．この時，100回＝1epochとなる．

## 6章

パラメータの更新方法には，SGDの他に有名なものとして，MomentumやAdaGrad，Adamなどの手法がある．
- SGD（確立的勾配降下法）  
勾配方向へある言っての距離だけ進む．
- Momentum  
ボールが地面の傾斜を転がるように動く．
- AdaGrad  
AdaGradのAdaは「適応的」を意味するAdaptiveに由来し，パラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法．
- Adam  
Momentumは，ボールがお椀を転がるように物理法則に準じる動き，AdaGradは，パラメータの要素ごとに，適応的に更新ステップを調整する手法．Adamは，両者を融合したような手法．